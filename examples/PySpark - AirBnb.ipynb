{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLeap.deploy() Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set-up running Spark 2.0 (required for this demo) from a Jupyter notebook, follow these [instructions](https://github.com/combust-ml/mleap/wiki/Setting-up-a-Spark-2.0-notebook-with-MLeap-and-Toree).\n",
    "\n",
    "This demo will show you how to:\n",
    "1. Load the research dataset from s3\n",
    "2. Construct a feature transformer pipeline using commonly available transformers in Spark\n",
    "3. Deploy your model to a public model server hosted on the combust.ml cloud using .deploy()\n",
    "\n",
    "NOTE: To run the actual deploy step you have to either:\n",
    "1. Get a key from combust.ml - it's easy, just email us!\n",
    "2. Fire up the combust cloud server on your local machine - also easy, send us an email and we'll send you a docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip > /dev/null\n",
    "!pip install findspark > /dev/null\n",
    "!pip install jip > /dev/null\n",
    "!pip install mleap > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for the demo was pulled together from individual cities' data found [here](http://insideairbnb.com/get-the-data.html). We've also gone ahead and pulled the individual datasets and relevant features into this [research dataset](https://s3-us-west-2.amazonaws.com/mleap-demo/datasources/airbnb.avro) stored as avro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Load libraries and data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mleap import pyspark\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  132M  100  132M    0     0  44.6M      0  0:00:02  0:00:02 --:--:-- 44.6M\n"
     ]
    }
   ],
   "source": [
    "!curl https://s3-us-west-2.amazonaws.com/mleap-demo/datasources/airbnb.avro -o /tmp/airbnb.avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   2 root hadoop  139027407 2018-11-13 17:33 /datasets/airbnb/airbnb.avro\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -mkdir -p /datasets/airbnb\n",
    "!hadoop fs -put /tmp/airbnb.avro /datasets/airbnb\n",
    "!hadoop fs -ls /datasets/airbnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('abc').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=u'1949687', name=u'Delectable Victorian Flat for two', space=u'A unique Victorian development of special architectural and historic interest as it combines housing and workshops. This is a thriving artist community, friendly and close to the centre of the city.', price=80.0, bathrooms=1.0, bedrooms=1.0, room_type=u'Entire home/apt', square_feet=None, host_is_superhost=0.0, city=u' London', state=u' London', cancellation_policy=u'moderate', security_deposit=100.0, cleaning_fee=20.0, extra_people=10.0, minimum_nights=3, first_review=u'2014-01-03', instant_bookable=0.0, number_of_reviews=8, review_scores_rating=94.0, price_per_bedroom=80.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"com.databricks.spark.avro\").load(\"/datasets/airbnb/airbnb.avro\")\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389255\n",
      "321588\n"
     ]
    }
   ],
   "source": [
    "datasetFiltered = df.filter(\"price >= 50 AND price <= 750 and bathrooms > 0.0\")\n",
    "print(df.count())\n",
    "print(datasetFiltered.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Standardize the data for our demo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|       square_feet|             price|          bedrooms|         bathrooms|     cleaning_fee|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            321588|            321588|            321588|            321588|           321588|\n",
      "|   mean| 546.7441757777032|131.54961006007687|1.3352426085550455| 1.199068373198005|37.64188340360959|\n",
      "| stddev|363.39839582373816| 90.10912788720013|0.8466586601060777|0.4830590051262755|42.64237791484603|\n",
      "|    min|             104.0|              50.0|               0.0|               0.5|              0.0|\n",
      "|    max|           32292.0|             750.0|              10.0|               8.0|            700.0|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasetFiltered.registerTempTable(\"df\")\n",
    "\n",
    "datasetImputed = spark.sql(\"\"\"\n",
    "    select\n",
    "        id,\n",
    "        city,\n",
    "        case when state in('NY', 'CA', 'London', 'Berlin', 'TX' ,'IL', 'OR', 'DC', 'WA')\n",
    "            then state\n",
    "            else 'Other'\n",
    "        end as state,\n",
    "        space,\n",
    "        price,\n",
    "        bathrooms,\n",
    "        bedrooms,\n",
    "        room_type,\n",
    "        host_is_superhost,\n",
    "        cancellation_policy,\n",
    "        case when security_deposit is null\n",
    "            then 0.0\n",
    "            else security_deposit\n",
    "        end as security_deposit,\n",
    "        price_per_bedroom,\n",
    "        case when number_of_reviews is null\n",
    "            then 0.0\n",
    "            else number_of_reviews\n",
    "        end as number_of_reviews,\n",
    "        case when extra_people is null\n",
    "            then 0.0\n",
    "            else extra_people\n",
    "        end as extra_people,\n",
    "        instant_bookable,\n",
    "        case when cleaning_fee is null\n",
    "            then 0.0\n",
    "            else cleaning_fee\n",
    "        end as cleaning_fee,\n",
    "        case when review_scores_rating is null\n",
    "            then 80.0\n",
    "            else review_scores_rating\n",
    "        end as review_scores_rating,\n",
    "        case when square_feet is not null and square_feet > 100\n",
    "            then square_feet\n",
    "            when (square_feet is null or square_feet <=100) and (bedrooms is null or bedrooms = 0)\n",
    "            then 350.0\n",
    "            else 380 * bedrooms\n",
    "        end as square_feet,\n",
    "        case when bathrooms >= 2\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as n_bathrooms_more_than_two\n",
    "    from df\n",
    "    where bedrooms is not null\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "datasetImputed.select(\"square_feet\", \"price\", \"bedrooms\", \"bathrooms\", \"cleaning_fee\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Take a look at some summary statistics of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+---------+---------+\n",
      "|        state|    n|avg_price|max_price|\n",
      "+-------------+-----+---------+---------+\n",
      "|           NY|48362|   146.75|    750.0|\n",
      "|           CA|44716|   158.76|    750.0|\n",
      "|Île-de-France|40732|   107.74|    750.0|\n",
      "|       London|17532|   117.71|    750.0|\n",
      "|          NSW|14416|   167.96|    750.0|\n",
      "|       Berlin|13098|    81.01|    650.0|\n",
      "|Noord-Holland| 8890|   128.56|    750.0|\n",
      "|          VIC| 8636|   144.49|    750.0|\n",
      "|North Holland| 7636|   134.60|    700.0|\n",
      "|           IL| 7544|   141.85|    750.0|\n",
      "|           ON| 7186|   129.05|    750.0|\n",
      "|           TX| 6702|   196.59|    750.0|\n",
      "|           WA| 5858|   132.48|    750.0|\n",
      "|    Catalonia| 5748|   106.39|    720.0|\n",
      "|           BC| 5522|   133.14|    750.0|\n",
      "|           DC| 5476|   136.56|    720.0|\n",
      "|       Québec| 5116|   104.98|    700.0|\n",
      "|    Catalunya| 4570|    99.36|    675.0|\n",
      "|       Veneto| 4486|   131.71|    700.0|\n",
      "|           OR| 4330|   114.02|    700.0|\n",
      "+-------------+-----+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most popular cities (original dataset)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        state,\n",
    "        count(*) as n,\n",
    "        cast(avg(price) as decimal(12,2)) as avg_price,\n",
    "        max(price) as max_price\n",
    "    from df\n",
    "    group by state\n",
    "    order by count(*) desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+---------+---------+\n",
      "|               city|  n|avg_price|max_price|\n",
      "+-------------------+---+---------+---------+\n",
      "|         Palm Beach| 56|   372.11|    701.0|\n",
      "|        Watsonville| 78|   307.85|    670.0|\n",
      "|  Pacific Palisades| 34|   295.18|    695.0|\n",
      "|             Malibu|302|   280.42|    750.0|\n",
      "|      Bilgola Beach| 30|   261.13|    601.0|\n",
      "|      Playa Del Rey| 34|   255.76|    599.0|\n",
      "|             Avalon| 80|   255.65|    701.0|\n",
      "|Sydney Olympic Park| 40|   250.55|    520.0|\n",
      "|           Tamarama|148|   247.45|    750.0|\n",
      "|           Capitola| 72|   246.50|    650.0|\n",
      "|    Manhattan Beach|240|   234.23|    700.0|\n",
      "|       Avalon Beach| 82|   232.98|    620.0|\n",
      "|            Del Mar| 38|   232.84|    650.0|\n",
      "|         Birchgrove| 32|   228.63|    601.0|\n",
      "|          Mona Vale| 52|   227.00|    572.0|\n",
      "|       Venice Beach| 62|   224.45|    699.0|\n",
      "|Rancho Palos Verdes| 82|   223.68|    750.0|\n",
      "|      Darling Point| 60|   221.43|    623.0|\n",
      "|    North Curl Curl| 26|   220.77|    550.0|\n",
      "|            Newport|114|   219.61|    750.0|\n",
      "+-------------------+---+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most expensive popular cities (original dataset)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        city,\n",
    "        count(*) as n,\n",
    "        cast(avg(price) as decimal(12,2)) as avg_price,\n",
    "        max(price) as max_price\n",
    "    from df\n",
    "    group by city\n",
    "    order by avg(price) desc\n",
    "\"\"\").filter(\"n>25\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define continous and categorical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Create our feature pipeline and train it on the entire dataset\n",
    "continuous_features = [\"bathrooms\", \"bedrooms\", \"security_deposit\", \"cleaning_fee\", \"extra_people\", \"number_of_reviews\", \"square_feet\", \"review_scores_rating\"]\n",
    "\n",
    "categorical_features = [\"room_type\", \"host_is_superhost\", \"cancellation_policy\", \"instant_bookable\", \"state\"]\n",
    "\n",
    "all_features = continuous_features + categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_imputed = datasetImputed.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split data into training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "[training_dataset, validation_dataset] = dataset_imputed.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Continous Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_feature_assembler= VectorAssembler(inputCols=continuous_features, outputCol=\"unscaled_continuous_features\")\n",
    "\n",
    "continuous_feature_scaler = StandardScaler(inputCol=\"unscaled_continuous_features\", outputCol=\"scaled_continuous_features\",\\\n",
    "                                           withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Categorical Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feature_indexers = [StringIndexer(inputCol=x, outputCol=\"{}_index\".format(x)) for x in categorical_features]\n",
    "\n",
    "categorical_feature_one_hot_encoders = [OneHotEncoder(inputCol=x.getOutputCol(), outputCol=\"oh_encoder_{}\".format(x.getOutputCol() )) for x in categorical_feature_indexers]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Assemble our features and feature pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished constructing the pipeline\n"
     ]
    }
   ],
   "source": [
    "estimatorsLr = [continuous_feature_assembler, continuous_feature_scaler] + categorical_feature_indexers+ categorical_feature_one_hot_encoders\n",
    "\n",
    "featurePipeline = Pipeline(stages=estimatorsLr)\n",
    "\n",
    "sparkFeaturePipelineModel = featurePipeline.fit(dataset_imputed)\n",
    "\n",
    "print(\"Finished constructing the pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train a Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: Training Linear Regression\n"
     ]
    }
   ],
   "source": [
    "# Create our linear regression model\n",
    "\n",
    "linearRegression = LinearRegression(featuresCol=\"scaled_continuous_features\", labelCol=\"price\", predictionCol=\"price_prediction\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "pipeline_lr = [sparkFeaturePipelineModel] + [linearRegression]\n",
    "\n",
    "sparkPipelineEstimatorLr = Pipeline(stages = pipeline_lr)\n",
    "\n",
    "sparkPipelineLr = sparkPipelineEstimatorLr.fit(dataset_imputed)\n",
    "\n",
    "print(\"Complete: Training Linear Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 0.8)\n",
      "epsilon: The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber (default: 1.35)\n",
      "featuresCol: features column name. (default: features, current: scaled_continuous_features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: price)\n",
      "loss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction, current: price_prediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.3)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.1: Train a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: Training Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "# Create our logistic regression model\n",
    "\n",
    "logisticRegression = LogisticRegression(featuresCol=\"scaled_continuous_features\", labelCol=\"n_bathrooms_more_than_two\", predictionCol=\"n_bathrooms_more_than_two_prediction\", maxIter=10)\n",
    "\n",
    "pipeline_log_r = [sparkFeaturePipelineModel] + [logisticRegression]\n",
    "\n",
    "sparkPipelineEstimatorLogr = Pipeline(stages = pipeline_log_r)\n",
    "\n",
    "sparkPipelineLogr = sparkPipelineEstimatorLogr.fit(dataset_imputed)\n",
    "\n",
    "print(\"Complete: Training Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id=u'1949687', city=u' London', state=u'Other', space=u'A unique Victorian development of special architectural and historic interest as it combines housing and workshops. This is a thriving artist community, friendly and close to the centre of the city.', price=80.0, bathrooms=1.0, bedrooms=1.0, room_type=u'Entire home/apt', host_is_superhost=0.0, cancellation_policy=u'moderate', security_deposit=100.0, price_per_bedroom=80.0, number_of_reviews=Decimal('8.0'), extra_people=10.0, instant_bookable=0.0, cleaning_fee=20.0, review_scores_rating=94.0, square_feet=380.0, n_bathrooms_more_than_two=Decimal('0.0'))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id=u'1949687', city=u' London', state=u'Other', space=u'A unique Victorian development of special architectural and historic interest as it combines housing and workshops. This is a thriving artist community, friendly and close to the centre of the city.', price=80.0, bathrooms=1.0, bedrooms=1.0, room_type=u'Entire home/apt', host_is_superhost=0.0, cancellation_policy=u'moderate', security_deposit=100.0, price_per_bedroom=80.0, number_of_reviews=Decimal('8.0'), extra_people=10.0, instant_bookable=0.0, cleaning_fee=20.0, review_scores_rating=94.0, square_feet=380.0, n_bathrooms_more_than_two=Decimal('0.0'))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id=u'1949687', city=u' London', state=u'Other', space=u'A unique Victorian development of special architectural and historic interest as it combines housing and workshops. This is a thriving artist community, friendly and close to the centre of the city.', price=80.0, bathrooms=1.0, bedrooms=1.0, room_type=u'Entire home/apt', host_is_superhost=0.0, cancellation_policy=u'moderate', security_deposit=100.0, price_per_bedroom=80.0, number_of_reviews=Decimal('8.0'), extra_people=10.0, instant_bookable=0.0, cleaning_fee=20.0, review_scores_rating=94.0, square_feet=380.0, n_bathrooms_more_than_two=Decimal('0.0'), unscaled_continuous_features=DenseVector([1.0, 1.0, 100.0, 20.0, 10.0, 8.0, 380.0, 94.0]), scaled_continuous_features=DenseVector([2.0701, 1.1811, 0.409, 0.469, 0.5352, 0.2859, 1.0457, 10.9593]), room_type_index=0.0, host_is_superhost_index=0.0, cancellation_policy_index=1.0, instant_bookable_index=0.0, state_index=0.0, oh_encoder_room_type_index=SparseVector(2, {0: 1.0}), oh_encoder_host_is_superhost_index=SparseVector(1, {0: 1.0}), oh_encoder_cancellation_policy_index=SparseVector(6, {1: 1.0}), oh_encoder_instant_bookable_index=SparseVector(1, {0: 1.0}), oh_encoder_state_index=SparseVector(9, {0: 1.0}), rawPrediction=DenseVector([7.5611, -7.5611]), probability=DenseVector([0.9995, 0.0005]), n_bathrooms_more_than_two_prediction=0.0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkPipelineLogr.transform(dataset_imputed).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Serialize the model to Bundle.ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkPipelineLr.serializeToBundle(\"jar:file:/tmp/pyspark.lr.zip\", sparkPipelineLr.transform(dataset_imputed))\n",
    "sparkPipelineLogr.serializeToBundle(\"jar:file:/tmp/pyspark.logr.zip\", dataset=sparkPipelineLogr.transform(dataset_imputed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9 (Optional): Deserialize from Bundle.ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkPipelineLr = PipelineModel.deserializeFromBundle(\"jar:file:/tmp/pyspark.lr.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10 (Optional): Deploy to ModelServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python bindings for .deploy() are coming soon. For now, you may have to write a few lines of scala - demo for that can be found [here](https://github.com/combust-ml/mleap-demo/blob/master/lending-club/notebooks/airbnb-price-regression.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}