{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLeap.deploy() Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set-up running Spark 2.0 (required for this demo) from a Jupyter notebook, follow these [instructions](https://github.com/combust-ml/mleap/wiki/Setting-up-a-Spark-2.0-notebook-with-MLeap-and-Toree).\n",
    "\n",
    "This demo will show you how to:\n",
    "1. Load the research dataset from s3\n",
    "2. Construct a feature transformer pipeline using commonly available transformers in Spark\n",
    "3. Deploy your model to a public model server hosted on the combust.ml cloud using .deploy()\n",
    "\n",
    "NOTE: To run the actual deploy step you have to either:\n",
    "1. Get a key from combust.ml - it's easy, just email us!\n",
    "2. Fire up the combust cloud server on your local machine - also easy, send us an email and we'll send you a docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jip > /dev/null\n",
    "!pip install mleap > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for the demo was pulled together from individual cities' data found [here](http://insideairbnb.com/get-the-data.html). We've also gone ahead and pulled the individual datasets and relevant features into this [research dataset](https://s3-us-west-2.amazonaws.com/mleap-demo/datasources/airbnb.avro) stored as avro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Load libraries and data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mleap import pyspark\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  132M  100  132M    0     0  46.2M      0  0:00:02  0:00:02 --:--:-- 46.2M\n"
     ]
    }
   ],
   "source": [
    "!curl https://s3-us-west-2.amazonaws.com/mleap-demo/datasources/airbnb.avro -o /tmp/airbnb.avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/11/07 19:33:28 INFO gcs.GoogleHadoopFileSystemBase: GHFS version: hadoop2-1.9.8\n",
      "18/11/07 19:33:30 INFO gcs.GoogleHadoopFileSystemBase: GHFS version: hadoop2-1.9.8\n",
      "put: `/datasets/airbnb/airbnb.avro': File exists\n",
      "18/11/07 19:33:33 INFO gcs.GoogleHadoopFileSystemBase: GHFS version: hadoop2-1.9.8\n",
      "Found 1 items\n",
      "-rw-r--r--   2 root hadoop  139027407 2018-11-07 19:23 /datasets/airbnb/airbnb.avro\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -mkdir -p /datasets/airbnb\n",
    "!hadoop fs -put /tmp/airbnb.avro /datasets/airbnb\n",
    "!hadoop fs -ls /datasets/airbnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('abc').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=u'1949687', name=u'Delectable Victorian Flat for two', space=u'A unique Victorian development of special architectural and historic interest as it combines housing and workshops. This is a thriving artist community, friendly and close to the centre of the city.', price=80.0, bathrooms=1.0, bedrooms=1.0, room_type=u'Entire home/apt', square_feet=None, host_is_superhost=0.0, city=u' London', state=u' London', cancellation_policy=u'moderate', security_deposit=100.0, cleaning_fee=20.0, extra_people=10.0, minimum_nights=3, first_review=u'2014-01-03', instant_bookable=0.0, number_of_reviews=8, review_scores_rating=94.0, price_per_bedroom=80.0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"com.databricks.spark.avro\").load(\"/datasets/airbnb/airbnb.avro\")\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389255\n",
      "321588\n"
     ]
    }
   ],
   "source": [
    "datasetFiltered = df.filter(\"price >= 50 AND price <= 750 and bathrooms > 0.0\")\n",
    "print(df.count())\n",
    "print(datasetFiltered.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Standardize the data for our demo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+-------------------+-----------------+\n",
      "|summary|      square_feet|             price|          bedrooms|          bathrooms|     cleaning_fee|\n",
      "+-------+-----------------+------------------+------------------+-------------------+-----------------+\n",
      "|  count|           321588|            321588|            321588|             321588|           321588|\n",
      "|   mean|546.7441757777032|131.54961006007687|1.3352426085550455|  1.199068373198005|37.64188340360959|\n",
      "| stddev| 363.398395823737| 90.10912788720113|0.8466586601060753|0.48305900512627775|42.64237791484587|\n",
      "|    min|            104.0|              50.0|               0.0|                0.5|              0.0|\n",
      "|    max|          32292.0|             750.0|              10.0|                8.0|            700.0|\n",
      "+-------+-----------------+------------------+------------------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasetFiltered.registerTempTable(\"df\")\n",
    "\n",
    "datasetImputed = spark.sql(\"\"\"\n",
    "    select\n",
    "        id,\n",
    "        city,\n",
    "        case when state in('NY', 'CA', 'London', 'Berlin', 'TX' ,'IL', 'OR', 'DC', 'WA')\n",
    "            then state\n",
    "            else 'Other'\n",
    "        end as state,\n",
    "        space,\n",
    "        price,\n",
    "        bathrooms,\n",
    "        bedrooms,\n",
    "        room_type,\n",
    "        host_is_superhost,\n",
    "        cancellation_policy,\n",
    "        case when security_deposit is null\n",
    "            then 0.0\n",
    "            else security_deposit\n",
    "        end as security_deposit,\n",
    "        price_per_bedroom,\n",
    "        case when number_of_reviews is null\n",
    "            then 0.0\n",
    "            else number_of_reviews\n",
    "        end as number_of_reviews,\n",
    "        case when extra_people is null\n",
    "            then 0.0\n",
    "            else extra_people\n",
    "        end as extra_people,\n",
    "        instant_bookable,\n",
    "        case when cleaning_fee is null\n",
    "            then 0.0\n",
    "            else cleaning_fee\n",
    "        end as cleaning_fee,\n",
    "        case when review_scores_rating is null\n",
    "            then 80.0\n",
    "            else review_scores_rating\n",
    "        end as review_scores_rating,\n",
    "        case when square_feet is not null and square_feet > 100\n",
    "            then square_feet\n",
    "            when (square_feet is null or square_feet <=100) and (bedrooms is null or bedrooms = 0)\n",
    "            then 350.0\n",
    "            else 380 * bedrooms\n",
    "        end as square_feet,\n",
    "        case when bathrooms >= 2\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as n_bathrooms_more_than_two\n",
    "    from df\n",
    "    where bedrooms is not null\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "datasetImputed.select(\"square_feet\", \"price\", \"bedrooms\", \"bathrooms\", \"cleaning_fee\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Take a look at some summary statistics of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+---------+---------+\n",
      "|        state|    n|avg_price|max_price|\n",
      "+-------------+-----+---------+---------+\n",
      "|           NY|48362|   146.75|    750.0|\n",
      "|           CA|44716|   158.76|    750.0|\n",
      "|Île-de-France|40732|   107.74|    750.0|\n",
      "|       London|17532|   117.71|    750.0|\n",
      "|          NSW|14416|   167.96|    750.0|\n",
      "|       Berlin|13098|    81.01|    650.0|\n",
      "|Noord-Holland| 8890|   128.56|    750.0|\n",
      "|          VIC| 8636|   144.49|    750.0|\n",
      "|North Holland| 7636|   134.60|    700.0|\n",
      "|           IL| 7544|   141.85|    750.0|\n",
      "|           ON| 7186|   129.05|    750.0|\n",
      "|           TX| 6702|   196.59|    750.0|\n",
      "|           WA| 5858|   132.48|    750.0|\n",
      "|    Catalonia| 5748|   106.39|    720.0|\n",
      "|           BC| 5522|   133.14|    750.0|\n",
      "|           DC| 5476|   136.56|    720.0|\n",
      "|       Québec| 5116|   104.98|    700.0|\n",
      "|    Catalunya| 4570|    99.36|    675.0|\n",
      "|       Veneto| 4486|   131.71|    700.0|\n",
      "|           OR| 4330|   114.02|    700.0|\n",
      "+-------------+-----+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most popular cities (original dataset)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        state,\n",
    "        count(*) as n,\n",
    "        cast(avg(price) as decimal(12,2)) as avg_price,\n",
    "        max(price) as max_price\n",
    "    from df\n",
    "    group by state\n",
    "    order by count(*) desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+---------+---------+\n",
      "|               city|  n|avg_price|max_price|\n",
      "+-------------------+---+---------+---------+\n",
      "|         Palm Beach| 56|   372.11|    701.0|\n",
      "|        Watsonville| 78|   307.85|    670.0|\n",
      "|  Pacific Palisades| 34|   295.18|    695.0|\n",
      "|             Malibu|302|   280.42|    750.0|\n",
      "|      Bilgola Beach| 30|   261.13|    601.0|\n",
      "|      Playa Del Rey| 34|   255.76|    599.0|\n",
      "|             Avalon| 80|   255.65|    701.0|\n",
      "|Sydney Olympic Park| 40|   250.55|    520.0|\n",
      "|           Tamarama|148|   247.45|    750.0|\n",
      "|           Capitola| 72|   246.50|    650.0|\n",
      "|    Manhattan Beach|240|   234.23|    700.0|\n",
      "|       Avalon Beach| 82|   232.98|    620.0|\n",
      "|            Del Mar| 38|   232.84|    650.0|\n",
      "|         Birchgrove| 32|   228.63|    601.0|\n",
      "|          Mona Vale| 52|   227.00|    572.0|\n",
      "|       Venice Beach| 62|   224.45|    699.0|\n",
      "|Rancho Palos Verdes| 82|   223.68|    750.0|\n",
      "|      Darling Point| 60|   221.43|    623.0|\n",
      "|    North Curl Curl| 26|   220.77|    550.0|\n",
      "|            Newport|114|   219.61|    750.0|\n",
      "+-------------------+---+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most expensive popular cities (original dataset)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        city,\n",
    "        count(*) as n,\n",
    "        cast(avg(price) as decimal(12,2)) as avg_price,\n",
    "        max(price) as max_price\n",
    "    from df\n",
    "    group by city\n",
    "    order by avg(price) desc\n",
    "\"\"\").filter(\"n>25\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define continous and categorical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Create our feature pipeline and train it on the entire dataset\n",
    "continuous_features = [\"bathrooms\", \"bedrooms\", \"security_deposit\", \"cleaning_fee\", \"extra_people\", \"number_of_reviews\", \"square_feet\", \"review_scores_rating\"]\n",
    "\n",
    "categorical_features = [\"room_type\", \"host_is_superhost\", \"cancellation_policy\", \"instant_bookable\", \"state\"]\n",
    "\n",
    "all_features = continuous_features + categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_imputed = datasetImputed.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split data into training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "[training_dataset, validation_dataset] = dataset_imputed.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Continous Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_feature_assembler= VectorAssembler(inputCols=continuous_features, outputCol=\"unscaled_continuous_features\")\n",
    "\n",
    "continuous_feature_scaler = StandardScaler(inputCol=\"unscaled_continuous_features\", outputCol=\"scaled_continuous_features\",\\\n",
    "                                           withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Categorical Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feature_indexers = [StringIndexer(inputCol=x, outputCol=\"{}_index\".format(x)) for x in categorical_features]\n",
    "\n",
    "categorical_feature_one_hot_encoders = [OneHotEncoder(inputCol=x.getOutputCol(), outputCol=\"oh_encoder_{}\".format(x.getOutputCol() )) for x in categorical_feature_indexers]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Assemble our features and feature pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished constructing the pipeline\n"
     ]
    }
   ],
   "source": [
    "estimatorsLr = [continuous_feature_assembler, continuous_feature_scaler] + categorical_feature_indexers+ categorical_feature_one_hot_encoders\n",
    "\n",
    "featurePipeline = Pipeline(stages=estimatorsLr)\n",
    "\n",
    "sparkFeaturePipelineModel = featurePipeline.fit(dataset_imputed)\n",
    "\n",
    "print(\"Finished constructing the pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train a Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: Training Linear Regression\n"
     ]
    }
   ],
   "source": [
    "# Create our linear regression model\n",
    "\n",
    "linearRegression = LinearRegression(featuresCol=\"scaled_continuous_features\", labelCol=\"price\", predictionCol=\"price_prediction\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "pipeline_lr = [sparkFeaturePipelineModel] + [linearRegression]\n",
    "\n",
    "sparkPipelineEstimatorLr = Pipeline(stages = pipeline_lr)\n",
    "\n",
    "sparkPipelineLr = sparkPipelineEstimatorLr.fit(dataset_imputed)\n",
    "\n",
    "print(\"Complete: Training Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.1: Train a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: Training Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "# Create our logistic regression model\n",
    "\n",
    "logisticRegression = LogisticRegression(featuresCol=\"scaled_continuous_features\", labelCol=\"n_bathrooms_more_than_two\", predictionCol=\"n_bathrooms_more_than_two_prediction\", maxIter=10)\n",
    "\n",
    "pipeline_log_r = [sparkFeaturePipelineModel] + [logisticRegression]\n",
    "\n",
    "sparkPipelineEstimatorLogr = Pipeline(stages = pipeline_log_r)\n",
    "\n",
    "sparkPipelineLogr = sparkPipelineEstimatorLogr.fit(dataset_imputed)\n",
    "\n",
    "print(\"Complete: Training Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Serialize the model to Bundle.ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2a3d7ad56339>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msparkPipelineLr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializeToBundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jar:file:/tmp/pyspark.lr.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparkPipelineLr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_imputed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msparkPipelineLogr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializeToBundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jar:file:/tmp/pyspark.logr.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparkPipelineLogr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_imputed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python2.7/site-packages/mleap/pyspark/spark_support.pyc\u001b[0m in \u001b[0;36mserializeToBundle\u001b[0;34m(self, path, dataset)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mserializeToBundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleSparkSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mserializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializeToBundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python2.7/site-packages/mleap/pyspark/spark_support.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSimpleSparkSerializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombust\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmleap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleSparkSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mserializeToBundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "sparkPipelineLr.serializeToBundle(\"jar:file:/tmp/pyspark.lr.zip\", sparkPipelineLr.transform(dataset_imputed))\n",
    "sparkPipelineLogr.serializeToBundle(\"jar:file:/tmp/pyspark.logr.zip\", dataset=sparkPipelineLogr.transform(dataset_imputed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9 (Optional): Deserialize from Bundle.ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkPipelineLr = PipelineModel.deserializeFromBundle(\"jar:file:/tmp/pyspark.lr.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10 (Optional): Deploy to ModelServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python bindings for .deploy() are coming soon. For now, you may have to write a few lines of scala - demo for that can be found [here](https://github.com/combust-ml/mleap-demo/blob/master/lending-club/notebooks/airbnb-price-regression.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}